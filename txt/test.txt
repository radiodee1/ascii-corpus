# Searching for Logical Ability in Large Language Models

This paper hopes to offer an explanation for some of the way that Large Language Models work. The paper does not offer proof, but rather a suggestion on how to think about transformer models.

## Small (Large) Language Models

There is something to observe when using models like gpt2 as a chatbot. If the models are overfit, if training goes on for too long, the models behave differently. They use a very small set of responses for all querys. If the models are trained for too long, they converge on a single sentence. In this example we will say that the model converged on the sentence "I don't know." The reason for this is that the phrase "I don't know" can be used as an answer for many questions. 

During training, before the massive overfitting, the model starts to limit it's replies drastically. At the start you can imagine replies of many kinds, but after a while, a while of overfitting, the output can be limited to a short list. The list has several entries and the number of entries goes down as the overfitting goes on. At some point the list can be very short. It consists of answers that apply very well to many inputs. Something like "I don't know" might be at the top of the list. You might find simple options like "Yes" and "No". 

## Lists 

What kind of lists are they? Are they like computer lists with indexes? Are they like linked lists? Are they heirarchical, like binary trees? Might there be a structure like a dictionary with keys and values? It would be helpful to have a definition or analogy for these lists.

The list that would be best is the tree, because it would be ordered with important replies at the top. Next might be the dictionary, followed by a linked list. There is not much evidence for any of these. There might be a dictionary relationship, but a tree would require some kind of index, as would a regular indexed list. There's not much evidence of an index anywhere. 

It might be that there is a list structure somewhere with empty spaces in it waiting for entries to be assigned. The entries are just close to each other. The thing that makes it a list is the proximity.

In the example the lists that are described in the overfitting of the gpt2 model are the output of the model. It is proposed that while overfitting the model the model shows it's inteligence. The items in the model themselves are not inteligent, but the making of the list, deciding where a given sentence belongs in the list, is smart. In this way even small models can show inteligence.

There is also an issue of input versus output. The list that we show above with the simple replies in a chatbot situation is an output list. One can also talk about internal lists, where a list inside the model exists that contains topics or concepts that are related. 

For example there are concepts of numbers. Speaking about the integers smaller than 10, there might be, for instance, special concepts for numbers like "1" and "2". These numbers can be used to mark text in paragraphs and can also be used for simple mathematical manipulation. "2" might also have special meaning with respect to the concept of a "pair". The word "pair" might need to be associated with the number "2" at times. This would give the digit "2" special meaning and also more than one meaning. Internally there might be a list for concepts around the number 2.

It's not known if lists are used internally in Large Language Models. It's not known how the input from a word is changed into a word of output. If there were lists in use, how would an item in a list be used? Would it be chosen somehow and would it somehow key a particular response? Would there be lists of lists? Wouldn't this be like the key/value dictionary mentioned above? 

It might be that the lists' values are simply close to each other. Their output might be similarly close to each other in a different dimension. It is easiest to immagine this in a 2 dimensional grid. In the vertical direction list items are saved. In the horizontal direction items in the list are connected to outputs. These outputs could be full words in this example but might be anything at all. Ultimately one could consider this arrangement in dimensions higher than two. 

## Training 

Does a model, even in the two dimensional example, learn by moving items farther apart from each other or does it place boundaries between items or concepts? It's not clear, but maybe examples should be presented.

Imagine a Scrabble board for this example. In regular Scrabble words start at some point on the board and proceed to the 'right' or in the 'down' direction. For this example we need words to start at a spot and proceed in the 'up' and 'left' direction also. In this example a word starts in some location and proceeds to it's end. Then up to three words can spring up at the tail of the word in question. The mechanism for a word to build a list at its termination and the later choosing from some part of that list remains unknown, but clearly there can only be three items in a list that is formed at the end of a word. There is no distance measurement in this scheme, and there are not boundaries erected between words or concepts.

In a different example, again using a Scrabble board, initial words start at the left margin of the board and only move in the 'right' direction. Subsequent lists start at the end of an initial word. Initial words are separated by spaces. They always start at the leftmost of the board. Words in the initial list are separated vertically by a space that allows the second sublist to exist. The distance between the members of the primary list is determined by the length of the list attached to the end of each primary member. It is not known how items are selected.

In another example, again using a Scrabble board, words are placed randomly on the board, never with regard to other words. The only rule is that no word destroys any other word. Words do not overlap. Some mechanism is used to identify words and to select words or letters for output, but it is unkown how it works. Somehow the model goes from one word to another, and there may be a complex relationship here. It could be a distance calculation. There could be a multiple dimensionality to this example, and there is also a limit to how many words can be chosen from. Note here that there needs to be a separation between words of one space at least at the end of a word if another word is going to exist on the same line.

This last example generalizes to one where multiple dimensions are mandatory to the discussion. Words or concepts exist in a large multiple dimension space. During training these words are moved away from each other in some dimensions and remain close to each other in other dimensions. The mechanism for choosing a item in a list is unkown.

Imagine a network with many remembered words. In order to allow one to lead to another the words might be close to each other. Then to dissacociate words or concepts we can either move the words apart or make some kind of barrier between words that cuts off their exchange. We don't know which is which.

There are easily other ways of visualizing how Large Language Models work. 

Is the process of building knowledge one where walls are built up between unrelated concepts, or is it a process of moving the concepts away from each other? The answer is not obvious. In the latter, with the possibility of many many dimensions, moving words farther from each other seems likely. In the former, building walls between objects seems to be wasteful. Some node needs to be sacrificed for another two nodes to be separated from each other. This last note seems intuitive, but in the overfit gpt2 model above there is a tendency for there to be fewer words in the set of outputs after overfitting than before. Maybe these words dissapear when they are converted to use as walls. Also in one of the Scrabble-based examples above, the only way to get the most words into the model was to separate them with blank spaces, as a wall would.

On the other hand, maybe distances are increased so much during a training proceedure that only a very few words are in a close enough relationship to each other that they can follow each other. Either way a large amount of data is lost with either arrangement, but the discussion at hand is for what happens during overfitting, not normal training.
